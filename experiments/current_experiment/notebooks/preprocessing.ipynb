{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading image dataset and CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas\n",
    "import numpy \n",
    "from augmentations import augmentations\n",
    "from datasets import datasets\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "images = []\n",
    "\n",
    "for fil in os.listdir(\"../data/raw_data/images\"):\n",
    "    file_dir = os.path.join(\"../data/raw/data/images\", fil)\n",
    "    images.append(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset information CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = numpy.vectorize(pyfunc=lambda img: Image.open(img))\n",
    "\n",
    "train_info = pandas.read_csv(\"../data/raw_data/information.csv\")\n",
    "train_info['image'] = image_loader(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info['Class'].map({\n",
    "    'yes': 0,\n",
    "    'no': 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting images to specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_png(img):\n",
    "    success, png_data = cv2.imencode(ext='.png', img=img)\n",
    "    if success:\n",
    "        png_img = cv2.imdecode(png_data, cv2.IMREAD_UNCHANGED)\n",
    "        return Image.fromarray(png_img)\n",
    "    else:\n",
    "        raise RuntimeError('Failed to convert image')\n",
    "\n",
    "train_info['image'] = train_info['image'].apply(lambda img: to_png(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking data for Gaussian Noise, Impulse Noise & Salt-And-Papper Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.restoration import estimate_sigma, denoise_tv_chambolle\n",
    "\n",
    "\n",
    "def has_gaussian_noise(img, threshold: float):\n",
    "    pass \n",
    "\n",
    "def has_salt_and_paper_noise(img, threshold: float):\n",
    "    pass \n",
    "\n",
    "def has_impulse_noise(img, threshold: int):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_info['image'].tolist()\n",
    "\n",
    "gaussian_thresh = 10 \n",
    "impulse_thresh = 0.8\n",
    "sp_thresh = 0.8\n",
    "\n",
    "noisy_images = []\n",
    "\n",
    "for image in images:\n",
    "    if (\n",
    "        has_gaussian_noise(image, gaussian_thresh) or \n",
    "        has_impulse_noise(image, impulse_thresh) or \n",
    "        has_salt_and_paper_noise(image, sp_thresh)\n",
    "    ):\n",
    "        noisy_images.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Datasets for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    numpy.arrange(train_info.shape[0]), \n",
    "    test_size=0.3, \n",
    "    stratify=train_info['class'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.DeepFakeClassificationDataset(\n",
    "    labels=train_info['class'][train_indices],\n",
    "    images=train_info['image'][train_indices],\n",
    "    transforms=augmentations.get_training_augmentations()\n",
    ")\n",
    "\n",
    "validation_dataset = datasets.DeepFakeClassificationDataset(\n",
    "    labels=train_info['class'][val_indices],\n",
    "    images=train_info['image'][val_indices],\n",
    "    transforms=augmentations.get_validation_augmentations()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post processing image quality estimation using SNR and SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ssim(trans_img, orig_img):\n",
    "    pass\n",
    "\n",
    "def estimate_snr(trans_img, orig_img):\n",
    "    pass\n",
    "\n",
    "for idx, img in enumerate(images):\n",
    "    snr, ssim = estimate_snr(), estimate_ssim()\n",
    "    print('%s: snr - %s; ssim - %s;', str(idx), snr, ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling disbalance in the dataset using Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_WEIGHTS = [1, 1]\n",
    "\n",
    "train_dataset.weights = validation_dataset.weights = CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving datasets to local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(obj=train_dataset, file=open(\"../data/augmented_data/train_dataset.pkl\", mode='wb'))\n",
    "pickle.dump(obj=validation_dataset, file=open(\"../data/augmented_data/validation_dataset.pkl\", mode='wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
