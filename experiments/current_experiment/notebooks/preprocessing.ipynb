{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading image dataset and CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas\n",
    "import numpy \n",
    "from augmentations import augmentations\n",
    "from datasets import datasets\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "images = []\n",
    "\n",
    "for fil in os.listdir(\"../data/raw_data/images\"):\n",
    "    file_dir = os.path.join(\"../data/raw/data/images\", fil)\n",
    "    images.append(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset information CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = numpy.vectorize(pyfunc=lambda img: Image.open(img))\n",
    "\n",
    "train_info = pandas.read_csv(\"../data/raw_data/information.csv\")\n",
    "train_info['image'] = image_loader(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info['Class'].map({\n",
    "    'yes': 0,\n",
    "    'no': 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting images to specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_png(img):\n",
    "    success, png_data = cv2.imencode(ext='.png', img=img)\n",
    "    if success:\n",
    "        png_img = cv2.imdecode(png_data, cv2.IMREAD_UNCHANGED)\n",
    "        return Image.fromarray(png_img)\n",
    "    else:\n",
    "        raise RuntimeError('Failed to convert image')\n",
    "\n",
    "train_info['image'] = train_info['image'].apply(lambda img: to_png(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking data for Gaussian Noise, Impulse Noise & Salt-And-Papper Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_gaussian_noise(img, threshold: float):\n",
    "    histogram = cv2.calcHist(\n",
    "        images=[img], \n",
    "        channels=[0],\n",
    "        mask=None, \n",
    "        ranges=[0, 256]\n",
    "    )  \n",
    "    return numpy.std(histogram) > threshold\n",
    "\n",
    "def has_salt_and_paper_noise(img, threshold: float):\n",
    "    low_d = numpy.count_nonzero(numpy.where(img == 0, 1, 0))\n",
    "    high_d = numpy.count_nonzero(numpy.where(img == 255, 1, 0))\n",
    "    return (low_d + high_d) / img.shape[0] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_info['image'].tolist()\n",
    "\n",
    "gaussian_thresh = 10 \n",
    "impulse_thresh = 0.8\n",
    "sp_thresh = 0.8\n",
    "\n",
    "sp_images = []\n",
    "gaussian_images = []\n",
    "\n",
    "for idx, image in enumerate(images):\n",
    "\n",
    "    if has_gaussian_noise(image, gaussian_thresh):\n",
    "        gaussian_images.append((idx, image))\n",
    "\n",
    "    if has_salt_and_paper_noise(image, sp_thresh):\n",
    "        sp_images.append((idx, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning image fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 10\n",
    "color_range = 20 \n",
    "space_range = 50\n",
    "\n",
    "for idx, img in [*gaussian_images, *sp_images]:\n",
    "    images[idx] = cv2.bilateralFilter(\n",
    "        src=img,\n",
    "        d=kernel_size,\n",
    "        sigmaColor=color_range,\n",
    "        sigma_space=space_range,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Datasets for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    numpy.arrange(train_info.shape[0]), \n",
    "    test_size=0.3, \n",
    "    stratify=train_info['class'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.DeepFakeClassificationDataset(\n",
    "    labels=train_info['class'][train_indices],\n",
    "    images=train_info['image'][train_indices],\n",
    "    transforms=augmentations.get_training_augmentations()\n",
    ")\n",
    "\n",
    "validation_dataset = datasets.DeepFakeClassificationDataset(\n",
    "    labels=train_info['class'][val_indices],\n",
    "    images=train_info['image'][val_indices],\n",
    "    transforms=augmentations.get_validation_augmentations()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling disbalance in the dataset using Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_WEIGHTS = [1, 1]\n",
    "\n",
    "train_dataset.weights = validation_dataset.weights = CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving datasets to local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(obj=train_dataset, file=open(\"../data/augmented_data/train_dataset.pkl\", mode='wb'))\n",
    "pickle.dump(obj=validation_dataset, file=open(\"../data/augmented_data/validation_dataset.pkl\", mode='wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
