{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f305666-4a1b-422c-b65a-14eb1c225828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e119a0eb-be29-4e26-a3e6-fcb818854a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab7707ff-9a9d-480a-b7e5-1baed346536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee28532c-85da-445e-a0c5-45237e014448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import json\n",
    "import pandas\n",
    "import typing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b87fa01-5c54-49d5-b490-abda03bc00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.losses import losses\n",
    "from src.training.metrics import metrics as eval_metrics\n",
    "from src.training.trainers import regularization\n",
    "from src.preprocessing import augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b30eaba-404d-4e16-b08f-72836e6a5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../experiments/experiment1/data/train_data\"\n",
    "TRAIN_ANNOTATIONS = \"../experiments/experiment1/labels/train_labels\"\n",
    "VALIDATION_DIR = \"../experiments/experiment1/data/validation_data\"\n",
    "VALIDATION_ANNOTATIONS = \"../experiments/experiment1/labels/validation_labels\"\n",
    "DATA_CONFIG_DIR = \"../experiments/experiment1/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b2d40-ff7d-40d8-bf5a-0005a8e18816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XMLAnnotationParser(object):\n",
    "    \"\"\"\n",
    "    Class for extracting annotations\n",
    "    from .xml files.\n",
    "    \"\"\"\n",
    "    def parse_annotations(self, input_path: str) -> pandas.DataFrame:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c7752d-cb31-45bb-85e9-a2f6dca7873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xml_annotations(annotations_path: str) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads xml annotations from the specified \n",
    "    'annotations_path' directory folder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        annotations_path - path, containing xml annotations for data\n",
    "    \"\"\"\n",
    "    parser = XMLAnnotationParser()\n",
    "    output_anns = None \n",
    "    \n",
    "    for path in glob(pathname=\"**/*.%s\" % file_ext, root_dir=annotations_path):\n",
    "        ann_path = os.path.join(annotations_path, path)\n",
    "        \n",
    "        annotations = parser.parse_annotations(ann_path)\n",
    "        if output_anns is None:\n",
    "            output_anns = annotations\n",
    "        else:\n",
    "            output_anns = pandas.concat([output_anns, annotations], axis=0)\n",
    "    return output_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef2882c-89e3-42db-8a77-9a9b5c67d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(root_dir: str, file_extensions: typing.List):\n",
    "    output_images = []\n",
    "    \n",
    "    for ext in file_extensions:\n",
    "        found_images = glob(pathname=\"**/*.%s\" % ext, root_dir=root_dir)\n",
    "        output_images.extend(found_images)\n",
    "        \n",
    "    return output_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94feb72-c38a-47d8-b602-a6703cb8b7e9",
   "metadata": {},
   "source": [
    "# Loading annotations for training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f65a5309-293e-4e74-bd91-dac6b7a670df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = load_xml_annotations(TRAIN_ANNOTATIONS)\n",
    "validation_annotations = load_xml_annotations(VALIDATION_ANNOTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba957427-8897-486b-8662-08b9f71acc43",
   "metadata": {},
   "source": [
    "# Loading image train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda729a-8470-497d-abe1-972ac33b5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_image_data(TRAIN_DATASET_DIR, [\"jpeg\", \"png\", \"jpg\"])\n",
    "validation_dataset = load_image_data(VALIDATION_DATASET_DIR, [\"jpeg\", \"png\", \"jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbcf932-83db-46c1-be9f-16b4f3e1c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashes = []\n",
    "train_annotations.set_index(\"video_name\")\n",
    "sorted_train_annotations = train_annotations.reindex(train_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df91ea-1dc2-4c34-b658-589c31aa5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hashes = []\n",
    "validation_annotations.set_index(\"video_name\")\n",
    "sorted_validation_annotations = validation_annotations.reindex(validation_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ba81-f972-458f-8e10-1485022e6656",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33601b92-d6a8-42c0-a9b7-48b14b9176d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentations = augmentations.get_train_augmentations(MTCNN_IMAGE_SIZE)\n",
    "validation_augmentations = augmentations.get_valid_augmentations(MTCNN_IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553da62-c2dd-40db-8686-69dc07c8eb57",
   "metadata": {},
   "source": [
    "# Initializing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76732b55-99d5-4120-8ff0-a28f7419c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = datasets.MTCNNFineTuneDataset(\n",
    "    image_paths=train_dataset,\n",
    "    boxes=sorted_train_annotations['boxes'].tolist(),\n",
    "    transformations=train_augmentations\n",
    ")\n",
    "\n",
    "validation_dataset = datasets.MTCNNFineTuneDataset(\n",
    "    image_paths=validation_dataset,\n",
    "    boxes=sorted_validation_annotations['boxes'].tolist(),\n",
    "    transformations=validation_augmentations\n",
    ")\n",
    "\n",
    "early_stop_dataset = datasets.MTCNNFineTuneDataset(\n",
    "    image_paths=early_dataset,\n",
    "    boxes=early_annotations['boxes'].tolist(),\n",
    "    transformations=validation_augmentations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d10c0-e54a-4092-b50a-123a2d8da74c",
   "metadata": {},
   "source": [
    "# Initializing configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc1b9c4-6f18-41b5-b50d-1fa7230cc1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.02\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# early stopping patience\n",
    "\n",
    "early_patience = 5\n",
    "min_diff = 0.05\n",
    "early_start = 2\n",
    "early_stopper = regularization.EarlyStopping(early_patience, min_diff)\n",
    "\n",
    "# initialization data loaders\n",
    "\n",
    "workers_per_loader = max(os.cpu_count()-1, 0) // 3 # for train, val and early loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "074e4a83-ffdb-4e48-aec7-f6ef682214e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval metric and loss function for fine-tuning\n",
    "loss_function = losses.CIOULoss()\n",
    "eval_metric = eval_metrics.IOUScore()\n",
    "\n",
    "# network, optimizer and LR Scheduling techniques\n",
    "\n",
    "network = MTCNN(\n",
    "    min_face_size=160, margin=10, \n",
    "    post_process=False, \n",
    "    thresholds=[0.8, 0.9, 0.95],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, [5, 10, 15, 20], gamma=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b875f92-82ac-4d5b-9891-f70bd7d0dd5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=workers_per_loader,\n",
    "    batch_size=train_batch_size\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    batch_size=1,\n",
    "    dataset=validation_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=workers_per_loader\n",
    ")\n",
    "\n",
    "early_loader = data.DataLoader(\n",
    "    batch_size=1,\n",
    "    dataset=early_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=workers_per_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a0c9254-6bae-4d73-bed2-2f6cda6e89bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59 µs, sys: 20 µs, total: 79 µs\n",
      "Wall time: 68.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class TunePipeline(object):\n",
    "    \"\"\"\n",
    "    Training pipeline for tuning\n",
    "    MTCNN face detector\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        network: nn.Module,\n",
    "        loss_function: nn.Module,\n",
    "        eval_metric: nn.Module,\n",
    "        max_epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer: nn.Module,\n",
    "        lr_scheduler: nn.Module,\n",
    "        snapshot_path: str,\n",
    "        inf_device\n",
    "    ):\n",
    "        self.network = network.to(device)\n",
    "        self.loss_function = loss_function\n",
    "        self.eval_metric = eval_metric\n",
    "        self.max_epochs: int = max_epochs\n",
    "        self.batch_size: int = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.snapshot_path = snapshot_path\n",
    "        self.inf_device = inf_device\n",
    "\n",
    "    def save_checkpoint(self, epoch: int, loss: float):\n",
    "        \n",
    "        snapshot_full_path = os.path.join(\n",
    "            self.snapshot_path, \n",
    "            \"model_epoch_%s.pth\" % epoch\n",
    "        )\n",
    "        \n",
    "        snapshot = {\n",
    "            'model_state': self.network.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'lr_scheduler_state': self.lr_scheduler.state_dict(),\n",
    "        }\n",
    "        torch.save(\n",
    "            obj=snapshot, \n",
    "            f=snapshot_full_path\n",
    "        )\n",
    "\n",
    "    def train(self, loader: data.DataLoader):\n",
    "        pass\n",
    "        \n",
    "    def evaluate(self, loader: data.DataLoader):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92323e-675f-4589-a2a4-1357671d6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer = TuningPipeline(\n",
    "    network=network,\n",
    "    loss_function=loss_function,\n",
    "    eval_metric=eval_metric,\n",
    "    max_epochs=max_epochs,\n",
    "    early_stopper=early_stopper,\n",
    "    early_dataset=early_dataset,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c7f20-1d56-4513-b40a-87a4ba486ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = trainer.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2f269-7cc0-440e-9f0e-ad110715ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss on fine tuning dataset: %s ' % train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db8761-f03b-4b52-9a05-24f8fec1f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = trainer.evaluate(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfea3cd-01f2-4cab-8cab-c9e7ffab09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('evaluation metric on validation dataset: %s ' % eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d973578-c506-4473-96d7-4ffaf0208edb",
   "metadata": {},
   "source": [
    "# Explaining MTCNN predictions manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0728a-2aa6-4be5-bb55-c20e3e674af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(ncols=2, nrows=3)\n",
    "\n",
    "for idx in range(6):\n",
    "    \n",
    "    img, _ = validation_dataset[idx]\n",
    "    output_img = img.copy()\n",
    "    boxes = trainer.predict(img)\n",
    "    \n",
    "    for box in range(len(boxes)):\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255,0,0), thickness=2)\n",
    "    ax[idx, 0].imshow(img)\n",
    "    ax[idx, 1].imshow(output_img)\n",
    "plt.imshow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
